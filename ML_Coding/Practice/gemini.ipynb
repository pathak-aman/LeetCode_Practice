{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToy Example: Basic End-to-End ML Workflow (Classification)\\n\\nDemonstrates the key steps for a timed ML technical interview scenario:\\n1. Load & Inspect Data\\n2. Basic EDA & Preprocessing Strategy\\n3. Train/Test Split\\n4. Preprocessing (Imputation, Encoding, Scaling) using Pipelines\\n5. Train Baseline Model\\n6. Evaluate Model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Toy Example: Basic End-to-End ML Workflow (Classification)\n",
    "\n",
    "Demonstrates the key steps for a timed ML technical interview scenario:\n",
    "1. Load & Inspect Data\n",
    "2. Basic EDA & Preprocessing Strategy\n",
    "3. Train/Test Split\n",
    "4. Preprocessing (Imputation, Encoding, Scaling) using Pipelines\n",
    "5. Train Baseline Model\n",
    "6. Evaluate Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Create & Load Toy Dataset ---\n",
    "# In a real scenario, you'd use pd.read_csv() or similar\n",
    "\n",
    "df = pd.read_csv(\"../data/kaggle_toy.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data ---\n",
      "   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
      "0   0  Mystery Matters    Episode 98                     NaN  True Crime   \n",
      "1   1    Joke Junction    Episode 26                  119.80      Comedy   \n",
      "2   2   Study Sessions    Episode 16                   73.90   Education   \n",
      "3   3   Digital Digest    Episode 45                   67.17  Technology   \n",
      "4   4      Mind & Body    Episode 86                  110.51      Health   \n",
      "\n",
      "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
      "0                       74.81        Thursday            Night   \n",
      "1                       66.95        Saturday        Afternoon   \n",
      "2                       69.97         Tuesday          Evening   \n",
      "3                       57.22          Monday          Morning   \n",
      "4                       80.07          Monday        Afternoon   \n",
      "\n",
      "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
      "0                          NaN            0.0          Positive   \n",
      "1                        75.95            2.0          Negative   \n",
      "2                         8.97            0.0          Negative   \n",
      "3                        78.70            2.0          Positive   \n",
      "4                        58.68            3.0           Neutral   \n",
      "\n",
      "   Listening_Time_minutes  \n",
      "0                31.41998  \n",
      "1                88.01241  \n",
      "2                44.92531  \n",
      "3                46.27824  \n",
      "4                75.61031  \n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           10000 non-null  int64  \n",
      " 1   Podcast_Name                 10000 non-null  object \n",
      " 2   Episode_Title                10000 non-null  object \n",
      " 3   Episode_Length_minutes       8825 non-null   float64\n",
      " 4   Genre                        10000 non-null  object \n",
      " 5   Host_Popularity_percentage   10000 non-null  float64\n",
      " 6   Publication_Day              10000 non-null  object \n",
      " 7   Publication_Time             10000 non-null  object \n",
      " 8   Guest_Popularity_percentage  8008 non-null   float64\n",
      " 9   Number_of_Ads                10000 non-null  float64\n",
      " 10  Episode_Sentiment            10000 non-null  object \n",
      " 11  Listening_Time_minutes       10000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 937.6+ KB\n",
      "None\n",
      "\n",
      "--- Basic Statistics (Numerical) ---\n",
      "                id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count  10000.00000             8825.000000                10000.000000   \n",
      "mean    4999.50000               64.798698                   59.940578   \n",
      "std     2886.89568               33.103045                   22.933476   \n",
      "min        0.00000                5.000064                   20.000000   \n",
      "25%     2499.75000               35.830000                   39.390000   \n",
      "50%     4999.50000               64.180000                   60.080000   \n",
      "75%     7499.25000               94.450000                   79.660000   \n",
      "max     9999.00000              119.960000                   99.960000   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
      "count                  8008.000000   10000.000000            10000.000000  \n",
      "mean                     51.662475       1.348500               45.695784  \n",
      "std                      28.302334       1.101621               27.268595  \n",
      "min                       0.030000       0.000000                0.000000  \n",
      "25%                      27.770000       0.000000               23.518887  \n",
      "50%                      52.715000       1.000000               43.600770  \n",
      "75%                      75.620000       2.000000               64.996592  \n",
      "max                     107.080000       3.000000              119.970000  \n",
      "\n",
      "--- Basic Statistics (Categorical) ---\n",
      "       Podcast_Name Episode_Title   Genre Publication_Day Publication_Time  \\\n",
      "count         10000         10000   10000           10000            10000   \n",
      "unique           48           100      10               7                4   \n",
      "top      Tech Talks    Episode 69  Sports          Sunday          Evening   \n",
      "freq            290           148    1221            1513             2688   \n",
      "\n",
      "       Episode_Sentiment  \n",
      "count              10000  \n",
      "unique                 3  \n",
      "top              Neutral  \n",
      "freq                3426  \n",
      "\n",
      "--- Missing Values ---\n",
      "id                                0\n",
      "Podcast_Name                      0\n",
      "Episode_Title                     0\n",
      "Episode_Length_minutes         1175\n",
      "Genre                             0\n",
      "Host_Popularity_percentage        0\n",
      "Publication_Day                   0\n",
      "Publication_Time                  0\n",
      "Guest_Popularity_percentage    1992\n",
      "Number_of_Ads                     0\n",
      "Episode_Sentiment                 0\n",
      "Listening_Time_minutes            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Initial Data ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Data Info ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Basic Statistics (Numerical) ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n--- Basic Statistics (Categorical) ---\")\n",
    "print(df.describe(include='object'))\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode_Sentiment\n",
       "Neutral     3426\n",
       "Negative    3345\n",
       "Positive    3229\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Episode_Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types (crucial for ColumnTransformer)\n",
    "# Note: In a real scenario with many columns, you might do this programmatically\n",
    "numerical_features = ['Guest_Popularity_percentage', 'Host_Popularity_percentage', \"Number_of_Ads\", \"Episode_Length_minutes\"]\n",
    "# One-Hot Encode 'Department' as it has no inherent order\n",
    "nominal_features = ['Genre', 'Publication_Day', 'Publication_Time']\n",
    "# Ordinal Encode 'ExperienceLevel' as it has a clear order\n",
    "ordinal_features = ['Episode_Sentiment']\n",
    "# Define the order for ordinal features\n",
    "experience_order = ['Negative', 'Neutral', 'Positive']\n",
    "removal_features = [\"id\",\"Podcast_Name\",\"Episode_Title\"]\n",
    "\n",
    "\n",
    "target = \"Listening_Time_minutes\"\n",
    "# --- 2. Define Features and Target ---\n",
    "X = df.drop(target, axis=1)\n",
    "X = df.drop(removal_features, axis = 1)\n",
    "y = df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Split ---\n",
      "Training set shape: (8000, 9)\n",
      "Test set shape: (2000, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Train/Test Split ---\n",
    "# Split *before* applying preprocessing that learns from data (like scaling or imputation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Stratify for classification is good practice\n",
    "print(f\"\\n--- Data Split ---\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "# Create pipeline for numerical features: Impute missing values with median, then scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create pipeline for nominal categorical features: Impute missing with most frequent, then one-hot encode\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output = False, drop = \"first\", handle_unknown='ignore')) # handle_unknown='ignore' is safer for unseen values in test set\n",
    "])\n",
    "\n",
    "# Create pipeline for ordinal categorical features: Impute missing with most frequent, then ordinal encode\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[experience_order])) # Pass the defined order\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Use ColumnTransformer to apply different transformers to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (if any) - 'drop' is also common\n",
    ").set_output(transform=\"pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__Guest_Popularity_percentage</th>\n",
       "      <th>num__Host_Popularity_percentage</th>\n",
       "      <th>num__Number_of_Ads</th>\n",
       "      <th>num__Episode_Length_minutes</th>\n",
       "      <th>nom__Genre_Comedy</th>\n",
       "      <th>nom__Genre_Education</th>\n",
       "      <th>nom__Genre_Health</th>\n",
       "      <th>nom__Genre_Lifestyle</th>\n",
       "      <th>nom__Genre_Music</th>\n",
       "      <th>nom__Genre_News</th>\n",
       "      <th>...</th>\n",
       "      <th>nom__Publication_Day_Saturday</th>\n",
       "      <th>nom__Publication_Day_Sunday</th>\n",
       "      <th>nom__Publication_Day_Thursday</th>\n",
       "      <th>nom__Publication_Day_Tuesday</th>\n",
       "      <th>nom__Publication_Day_Wednesday</th>\n",
       "      <th>nom__Publication_Time_Evening</th>\n",
       "      <th>nom__Publication_Time_Morning</th>\n",
       "      <th>nom__Publication_Time_Night</th>\n",
       "      <th>ord__Episode_Sentiment</th>\n",
       "      <th>remainder__Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>-0.014637</td>\n",
       "      <td>-0.147502</td>\n",
       "      <td>1.495380</td>\n",
       "      <td>-1.362223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.02668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>0.420455</td>\n",
       "      <td>-1.246593</td>\n",
       "      <td>-1.227205</td>\n",
       "      <td>-0.925938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.01009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>0.527362</td>\n",
       "      <td>-0.837750</td>\n",
       "      <td>1.495380</td>\n",
       "      <td>-0.018595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.97471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>-1.548269</td>\n",
       "      <td>1.032923</td>\n",
       "      <td>-0.319677</td>\n",
       "      <td>1.295088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>82.81752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>-0.210370</td>\n",
       "      <td>1.349559</td>\n",
       "      <td>-1.227205</td>\n",
       "      <td>-0.834496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.35078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.667079</td>\n",
       "      <td>-0.112707</td>\n",
       "      <td>-0.319677</td>\n",
       "      <td>1.101255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.31980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>0.030562</td>\n",
       "      <td>-0.182732</td>\n",
       "      <td>-0.319677</td>\n",
       "      <td>-0.018595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0.030562</td>\n",
       "      <td>1.068588</td>\n",
       "      <td>-1.227205</td>\n",
       "      <td>1.576499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.64095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-1.210257</td>\n",
       "      <td>1.579206</td>\n",
       "      <td>1.495380</td>\n",
       "      <td>-0.798756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.47442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>0.030562</td>\n",
       "      <td>-1.081316</td>\n",
       "      <td>-1.227205</td>\n",
       "      <td>1.124438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.11876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num__Guest_Popularity_percentage  num__Host_Popularity_percentage  \\\n",
       "9254                         -0.014637                        -0.147502   \n",
       "1561                          0.420455                        -1.246593   \n",
       "1670                          0.527362                        -0.837750   \n",
       "6087                         -1.548269                         1.032923   \n",
       "6669                         -0.210370                         1.349559   \n",
       "...                                ...                              ...   \n",
       "5734                         -0.667079                        -0.112707   \n",
       "5191                          0.030562                        -0.182732   \n",
       "5390                          0.030562                         1.068588   \n",
       "860                          -1.210257                         1.579206   \n",
       "7270                          0.030562                        -1.081316   \n",
       "\n",
       "      num__Number_of_Ads  num__Episode_Length_minutes  nom__Genre_Comedy  \\\n",
       "9254            1.495380                    -1.362223                0.0   \n",
       "1561           -1.227205                    -0.925938                0.0   \n",
       "1670            1.495380                    -0.018595                1.0   \n",
       "6087           -0.319677                     1.295088                1.0   \n",
       "6669           -1.227205                    -0.834496                0.0   \n",
       "...                  ...                          ...                ...   \n",
       "5734           -0.319677                     1.101255                0.0   \n",
       "5191           -0.319677                    -0.018595                1.0   \n",
       "5390           -1.227205                     1.576499                0.0   \n",
       "860             1.495380                    -0.798756                0.0   \n",
       "7270           -1.227205                     1.124438                0.0   \n",
       "\n",
       "      nom__Genre_Education  nom__Genre_Health  nom__Genre_Lifestyle  \\\n",
       "9254                   0.0                1.0                   0.0   \n",
       "1561                   0.0                0.0                   0.0   \n",
       "1670                   0.0                0.0                   0.0   \n",
       "6087                   0.0                0.0                   0.0   \n",
       "6669                   1.0                0.0                   0.0   \n",
       "...                    ...                ...                   ...   \n",
       "5734                   0.0                0.0                   0.0   \n",
       "5191                   0.0                0.0                   0.0   \n",
       "5390                   1.0                0.0                   0.0   \n",
       "860                    0.0                0.0                   0.0   \n",
       "7270                   0.0                1.0                   0.0   \n",
       "\n",
       "      nom__Genre_Music  nom__Genre_News  ...  nom__Publication_Day_Saturday  \\\n",
       "9254               0.0              0.0  ...                            0.0   \n",
       "1561               0.0              0.0  ...                            0.0   \n",
       "1670               0.0              0.0  ...                            0.0   \n",
       "6087               0.0              0.0  ...                            0.0   \n",
       "6669               0.0              0.0  ...                            0.0   \n",
       "...                ...              ...  ...                            ...   \n",
       "5734               0.0              0.0  ...                            0.0   \n",
       "5191               0.0              0.0  ...                            0.0   \n",
       "5390               0.0              0.0  ...                            1.0   \n",
       "860                0.0              0.0  ...                            1.0   \n",
       "7270               0.0              0.0  ...                            0.0   \n",
       "\n",
       "      nom__Publication_Day_Sunday  nom__Publication_Day_Thursday  \\\n",
       "9254                          0.0                            0.0   \n",
       "1561                          1.0                            0.0   \n",
       "1670                          0.0                            0.0   \n",
       "6087                          0.0                            1.0   \n",
       "6669                          1.0                            0.0   \n",
       "...                           ...                            ...   \n",
       "5734                          0.0                            0.0   \n",
       "5191                          0.0                            0.0   \n",
       "5390                          0.0                            0.0   \n",
       "860                           0.0                            0.0   \n",
       "7270                          1.0                            0.0   \n",
       "\n",
       "      nom__Publication_Day_Tuesday  nom__Publication_Day_Wednesday  \\\n",
       "9254                           0.0                             0.0   \n",
       "1561                           0.0                             0.0   \n",
       "1670                           1.0                             0.0   \n",
       "6087                           0.0                             0.0   \n",
       "6669                           0.0                             0.0   \n",
       "...                            ...                             ...   \n",
       "5734                           1.0                             0.0   \n",
       "5191                           0.0                             1.0   \n",
       "5390                           0.0                             0.0   \n",
       "860                            0.0                             0.0   \n",
       "7270                           0.0                             0.0   \n",
       "\n",
       "      nom__Publication_Time_Evening  nom__Publication_Time_Morning  \\\n",
       "9254                            0.0                            0.0   \n",
       "1561                            1.0                            0.0   \n",
       "1670                            0.0                            1.0   \n",
       "6087                            0.0                            0.0   \n",
       "6669                            0.0                            1.0   \n",
       "...                             ...                            ...   \n",
       "5734                            0.0                            0.0   \n",
       "5191                            0.0                            1.0   \n",
       "5390                            1.0                            0.0   \n",
       "860                             0.0                            0.0   \n",
       "7270                            0.0                            1.0   \n",
       "\n",
       "      nom__Publication_Time_Night  ord__Episode_Sentiment  \\\n",
       "9254                          1.0                     2.0   \n",
       "1561                          0.0                     0.0   \n",
       "1670                          0.0                     2.0   \n",
       "6087                          0.0                     2.0   \n",
       "6669                          0.0                     1.0   \n",
       "...                           ...                     ...   \n",
       "5734                          1.0                     0.0   \n",
       "5191                          0.0                     1.0   \n",
       "5390                          0.0                     0.0   \n",
       "860                           1.0                     0.0   \n",
       "7270                          0.0                     1.0   \n",
       "\n",
       "      remainder__Listening_Time_minutes  \n",
       "9254                            7.02668  \n",
       "1561                           25.01009  \n",
       "1670                           23.97471  \n",
       "6087                           82.81752  \n",
       "6669                           33.35078  \n",
       "...                                 ...  \n",
       "5734                           65.31980  \n",
       "5191                           95.50000  \n",
       "5390                           69.64095  \n",
       "860                            27.47442  \n",
       "7270                           51.11876  \n",
       "\n",
       "[8000 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tr = preprocessor.fit_transform(X_train)\n",
    "X_train_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting lr Model Training ---\n",
      "--- lr Model Training Complete ---\n",
      "Evaluating\n",
      "3.7066658918417053e-28\n",
      "\n",
      "--- Starting rf Model Training ---\n",
      "--- rf Model Training Complete ---\n",
      "Evaluating\n",
      "0.00031213054222578736\n",
      "\n",
      "--- Starting SVR Model Training ---\n",
      "--- SVR Model Training Complete ---\n",
      "Evaluating\n",
      "0.17093510220317884\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Create Full Pipeline with Model ---\n",
    "# Choose a simple baseline model\n",
    "models = {\n",
    "    \"lr\" : LinearRegression(),\n",
    "    \"rf\" : RandomForestRegressor(random_state=42),\n",
    "    \"SVR\" : SVR(),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "# Chain the preprocessor and the model into a single pipeline\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    print(f\"\\n--- Starting {name} Model Training ---\")\n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    print(f\"--- {name} Model Training Complete ---\")\n",
    "    print(\"Evaluating\")\n",
    "    y_pred = full_pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on the test data\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1] # Get probabilities for AUC if needed\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "# Note: With a tiny dataset like this, the report might look sparse or have warnings\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary & Next Steps ---\n",
      "Successfully trained a baseline Logistic Regression model.\n",
      "Achieved an accuracy of 0.1709 on the unseen test data.\n",
      "Next steps if more time allowed:\n",
      "- More detailed EDA (visualizations, outlier detection/handling).\n",
      "- Experiment with different imputation strategies (e.g., KNNImputer).\n",
      "- Feature engineering (e.g., interaction terms like Age*Salary).\n",
      "- Try more complex models (e.g., RandomForest, GradientBoosting).\n",
      "- Hyperparameter tuning using GridSearchCV or RandomizedSearchCV with Cross-Validation.\n",
      "- Deeper error analysis (examining misclassified examples).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 8. Interpretation & Next Steps (Simulated) ---\n",
    "print(\"\\n--- Summary & Next Steps ---\")\n",
    "print(f\"Successfully trained a baseline Logistic Regression model.\")\n",
    "print(f\"Achieved an accuracy of {mse:.4f} on the unseen test data.\")\n",
    "print(\"Next steps if more time allowed:\")\n",
    "print(\"- More detailed EDA (visualizations, outlier detection/handling).\")\n",
    "print(\"- Experiment with different imputation strategies (e.g., KNNImputer).\")\n",
    "print(\"- Feature engineering (e.g., interaction terms like Age*Salary).\")\n",
    "print(\"- Try more complex models (e.g., RandomForest, GradientBoosting).\")\n",
    "print(\"- Hyperparameter tuning using GridSearchCV or RandomizedSearchCV with Cross-Validation.\")\n",
    "print(\"- Deeper error analysis (examining misclassified examples).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
