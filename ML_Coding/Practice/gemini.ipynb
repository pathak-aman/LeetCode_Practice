{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Toy Example: Basic End-to-End ML Workflow (Classification)\n",
    "\n",
    "Demonstrates the key steps for a timed ML technical interview scenario:\n",
    "1. Load & Inspect Data\n",
    "2. Basic EDA & Preprocessing Strategy\n",
    "3. Train/Test Split\n",
    "4. Preprocessing (Imputation, Encoding, Scaling) using Pipelines\n",
    "5. Train Baseline Model\n",
    "6. Evaluate Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Create & Load Toy Dataset ---\n",
    "# In a real scenario, you'd use pd.read_csv() or similar\n",
    "data = {\n",
    "    'Age': [25, 45, 35, 50, 23, np.nan, 60, 70, 31, 41],\n",
    "    'Salary': [50000, 80000, 60000, 120000, 45000, 55000, np.nan, 150000, 52000, 75000],\n",
    "    'Department': ['HR', 'IT', 'Marketing', 'IT', 'HR', 'Marketing', 'Finance', 'IT', np.nan, 'Marketing'],\n",
    "    'ExperienceLevel': ['Entry', 'Senior', 'Mid', 'Senior', 'Entry', 'Mid', 'Senior', 'Senior', 'Mid', 'Mid'],\n",
    "    # Target variable: 1 if likely to churn, 0 otherwise\n",
    "    'Churn': [0, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initial Data ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Data Info ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Basic Statistics (Numerical) ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n--- Basic Statistics (Categorical) ---\")\n",
    "print(df.describe(include='object'))\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n--- Target Variable Distribution ---\")\n",
    "print(df['Churn'].value_counts(normalize=True)) # Use normalize=True for proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 2. Define Features and Target ---\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Identify feature types (crucial for ColumnTransformer)\n",
    "# Note: In a real scenario with many columns, you might do this programmatically\n",
    "numerical_features = ['Age', 'Salary']\n",
    "# One-Hot Encode 'Department' as it has no inherent order\n",
    "nominal_features = ['Department']\n",
    "# Ordinal Encode 'ExperienceLevel' as it has a clear order\n",
    "ordinal_features = ['ExperienceLevel']\n",
    "# Define the order for ordinal features\n",
    "experience_order = ['Entry', 'Mid', 'Senior']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Train/Test Split ---\n",
    "# Split *before* applying preprocessing that learns from data (like scaling or imputation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) # Stratify for classification is good practice\n",
    "print(f\"\\n--- Data Split ---\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "# Create pipeline for numerical features: Impute missing values with median, then scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create pipeline for nominal categorical features: Impute missing with most frequent, then one-hot encode\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is safer for unseen values in test set\n",
    "])\n",
    "\n",
    "# Create pipeline for ordinal categorical features: Impute missing with most frequent, then ordinal encode\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[experience_order])) # Pass the defined order\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Use ColumnTransformer to apply different transformers to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (if any) - 'drop' is also common\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 5. Create Full Pipeline with Model ---\n",
    "# Choose a simple baseline model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Chain the preprocessor and the model into a single pipeline\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "# --- 6. Train the Model ---\n",
    "# Fit the entire pipeline on the training data\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "print(\"--- Model Training Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7. Evaluate the Model ---\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "# Predict on the test data\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1] # Get probabilities for AUC if needed\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "# Note: With a tiny dataset like this, the report might look sparse or have warnings\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 8. Interpretation & Next Steps (Simulated) ---\n",
    "print(\"\\n--- Summary & Next Steps ---\")\n",
    "print(f\"Successfully trained a baseline Logistic Regression model.\")\n",
    "print(f\"Achieved an accuracy of {accuracy:.4f} on the unseen test data.\")\n",
    "print(\"Next steps if more time allowed:\")\n",
    "print(\"- More detailed EDA (visualizations, outlier detection/handling).\")\n",
    "print(\"- Experiment with different imputation strategies (e.g., KNNImputer).\")\n",
    "print(\"- Feature engineering (e.g., interaction terms like Age*Salary).\")\n",
    "print(\"- Try more complex models (e.g., RandomForest, GradientBoosting).\")\n",
    "print(\"- Hyperparameter tuning using GridSearchCV or RandomizedSearchCV with Cross-Validation.\")\n",
    "print(\"- Deeper error analysis (examining misclassified examples).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
